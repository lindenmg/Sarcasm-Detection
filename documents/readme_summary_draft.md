# Sarcasm Detection Project

## Overview and Task

This project aims to develop a system capable of detecting sarcasm in web forum comments using Deep Neural Networks. It was conducted by Gabriel Lindenmaier from the University of Stuttgart. We built classifiers using Deep Neural Networks to process sarcastic comments from the SARC corpus, which contains Reddit posts. Our CNN achieved human-level performance despite noisy data. An unoptimized convolution with attention performed slightly better than normal convolution and served as a good feature generator for other Machine Learning (ML) classifiers.

The primary task was to classify replies as sarcastic or not using Deep Learning. PyTorch was used to implement the Neural Networks. The research question focused on combining convolution layers with attention and using the CNN as a feature generator for other ML classifiers.

## Data
The dataset is based on the Self-Annotated Corpus for Sarcasm (SARC) from Princeton University, containing Reddit posts annotated for sarcasm. Key points:
- Evenly distributed sarcastic and non-sarcastic replies
- Preprocessed to remove noise and convert text to numeric representation
- Training, validation, and test sets split as follows:
  - Train: 196,526 pairs
  - Validation: 21,836 pairs
  - Test: 21,836 pairs

## Baseline
Our baseline model is a Convolutional Neural Network (CNN) with:
- Preprocessing steps converting text to numeric vectors
- Embedding layer with pretrained fastText vectors
- One convolutional layer followed by a fully connected layer

### Preprocessing Steps
- Tokenization, lowercasing, and special character transformation
- Use of pretrained fastText word vectors
- Handling of numeric tokens, emails, and URLs

### CNN Model Architecture
- Separate inputs for posts and replies
- Embedding layer, 2D convolution, PReLU activation, and 1-max pooling
- Cosine similarity calculation and fully connected layer for classification

## Research Question
The research question involved:
- Implementing convolution with attention
- Using the CNN's output as features for ML classifiers like Logistic Regression, Random Forest, and Extremely Randomized Trees

### Convolution with Attention
Inspired by Yin et al., we replaced the convolution layer with an attention mechanism, reducing overfitting and improving results.

### ML Classifiers
Features generated by the CNN with attention were used to train various ML classifiers. These classifiers performed slightly better than the CNN's fully connected layer.

## Results and Analysis
- Baseline CNN achieved ~67.6% accuracy
- CNN with attention achieved ~67.82% accuracy
- ML classifiers based on CNN features achieved up to 68.64% accuracy
- A survey comparing human performance showed the CNN performs comparably to humans

## Conclusion
The project demonstrated that deep learning models could effectively detect sarcasm in text. Future work could focus on optimizing the attention mechanism and improving data quality by retaining links and better annotating non-sarcastic posts.
