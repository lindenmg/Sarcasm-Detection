{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "os.chdir('/home/noname/Documents/Studium/Master/2_Semester/Deep_Learning_fSaLP/Project')\n",
    "\n",
    "from src.preprocessing.preprocessing import Preprocessing\n",
    "from src.data_science.networkhelper import NetworkHelper\n",
    "from src.preprocessing.datahandler import DataHandler\n",
    "import src.tools.helpers as helpers\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_train_test(data_dir, dh):\n",
    "    train_file = data_dir / \"train.csv\"\n",
    "    test_file = data_dir / \"test.csv\"\n",
    "    if not (train_file.is_file() or test_file.is_file()):\n",
    "        comments = str(data_dir / \"comments_cleaned.txt\")\n",
    "        annotation = str(data_dir / \"annotation.txt\")\n",
    "        dh.load_data(comments, annotation)\n",
    "        dh.split_in_train_test()\n",
    "        dh.save_train_test_to_csv(str(data_dir))\n",
    "    else:\n",
    "        dh.load_train_test(str(data_dir))\n",
    "    train = dh.get_train_df(deep_copy=False)\n",
    "    test = dh.get_test_df(deep_copy=False)\n",
    "    return test, train"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "data_dir = Path('data')\n",
    "sw_cut_file = str(data_dir / 'stop_words_cut_ultra.txt')\n",
    "sw_full_file = str(data_dir / 'stop_words_full_ultra.txt')\n",
    "nh = NetworkHelper()\n",
    "dh = DataHandler()\n",
    "test, train = load_train_test(data_dir, dh)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "max_post_len = 30"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "pp = Preprocessing()\n",
    "nlp = pp.get_nlp()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_spacy_pipeline(post_path, reply_path, df):\n",
    "    post_dump = Path(post_path)\n",
    "    reply_dump = Path(reply_path)\n",
    "    if not post_dump.is_file():\n",
    "        posts = pp.run_spacy_pipeline(df['post'][0::2])\n",
    "        helpers.save_to_disk(posts, post_path)\n",
    "    else:\n",
    "        posts = helpers.load_from_disk(post_path)\n",
    "    if not reply_dump.is_file():\n",
    "        replies = pp.run_spacy_pipeline(df['reply'])\n",
    "        helpers.save_to_disk(replies, reply_path)\n",
    "    else:\n",
    "        replies = helpers.load_from_disk(reply_path)\n",
    "    return posts, replies\n",
    "\n",
    "\n",
    "def apply_token_to_x(post_path, reply_path, posts, replies, type_):\n",
    "    post_dump = Path(post_path)\n",
    "    reply_dump = Path(reply_path)\n",
    "    if not post_dump.is_file():\n",
    "        nlp.add_stop_word_def(sw_full_file)\n",
    "        post_docs = pp.filter_spacy_tokens(posts, no_stop_words=False, no_punctuation=False)\n",
    "        post_pcd = pp.convert_token_docs_text(post_docs, token_kind=type_, transform_specials=True)\n",
    "        helpers.save_to_disk(post_pcd, post_path)\n",
    "    else:\n",
    "        post_pcd = helpers.load_from_disk(post_path)\n",
    "    if not reply_dump.is_file():\n",
    "        nlp.add_stop_word_def(sw_cut_file)\n",
    "        reply_docs = pp.filter_spacy_tokens(replies, no_stop_words=False, no_punctuation=False)\n",
    "        reply_pcd = pp.convert_token_docs_text(reply_docs, token_kind=type_, transform_specials=True)\n",
    "        helpers.save_to_disk(reply_pcd, reply_path)\n",
    "    else:\n",
    "        reply_pcd = helpers.load_from_disk(reply_path)\n",
    "    return post_pcd, reply_pcd\n",
    "\n",
    "\n",
    "def conv_str_to_emb_idx(post_path, reply_path, posts, replies, word_idx, max_len=1000):\n",
    "    post_dump = Path(post_path)\n",
    "    reply_dump = Path(reply_path)\n",
    "    if not (post_dump.is_file() and reply_dump.is_file()):\n",
    "        post_emb = nh.convert_str_to_emb_idx(posts, word_idx, max_len)\n",
    "        reply_emb = nh.convert_str_to_emb_idx(replies, word_idx, max_len)\n",
    "        helpers.save_to_disk(post_emb, post_path)\n",
    "        helpers.save_to_disk(reply_emb, reply_path)\n",
    "    else:\n",
    "        post_emb = helpers.load_from_disk(post_path)\n",
    "        reply_emb = helpers.load_from_disk(reply_path)\n",
    "    return post_emb, reply_emb\n",
    "\n",
    "\n",
    "def get_labels(train_path, test_path, train, test):\n",
    "    train_dump = Path(train_path)\n",
    "    test_dump = Path(test_path)\n",
    "    if not (train_dump.is_file() and test_dump.is_file()):\n",
    "        train = train.values.astype(dtype=np.long, copy=False)\n",
    "        test = test.values.astype(dtype=np.long, copy=False)\n",
    "        train = torch.from_numpy(train)\n",
    "        test = torch.from_numpy(test)\n",
    "        helpers.save_to_disk(train, train_path)\n",
    "        helpers.save_to_disk(test, test_path)\n",
    "    else:\n",
    "        train = helpers.load_from_disk(train_path)\n",
    "        test = helpers.load_from_disk(test_path)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def get_length_tensor(replies, posts):\n",
    "    reply_length = helpers.create_length_tensor(replies)\n",
    "    post_length = helpers.create_length_tensor(posts)\n",
    "    return post_length, reply_length"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "posts_train, reply_train = apply_spacy_pipeline('data/posts.pkl', 'data/replies.pkl', train)\n",
    "# posts_train = None\n",
    "# reply_train = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "posts_test, reply_test = apply_spacy_pipeline('data/posts_test.pkl', 'data/replies_test.pkl', test)\n",
    "# reply_test = None\n",
    "# posts_test = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "post_conv_train, reply_conv_train = apply_token_to_x('data/post_lower.pkl', 'data/reply_lower.pkl'\n",
    "                                                     , posts_train, reply_train, 'lower_')\n",
    "post_conv_test, reply_conv_test = apply_token_to_x('data/post_lower_test.pkl'\n",
    "                                                   , 'data/reply_lower_test.pkl'\n",
    "                                                   , posts_test, reply_test, 'lower_')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "post_conv_train, reply_conv_train = apply_token_to_x('data/post_text_train.pkl', 'data/reply_text_train.pkl'\n",
    "                                                           , posts_train, reply_train, 'text')\n",
    "post_conv_test, reply_conv_test = apply_token_to_x('data/post_text_test.pkl'\n",
    "                                                         , 'data/reply_text_test.pkl'\n",
    "                                                         , posts_test, reply_test, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "post_feats_tr, _ = pp.filter_by_frequency(post_conv_train, min_freq=3)\n",
    "reply_feats_tr, _ = pp.filter_by_frequency(reply_conv_train, min_freq=3)\n",
    "post_feats_te, _ = pp.filter_by_frequency(post_conv_test, min_freq=3)\n",
    "reply_feats_te, _ = pp.filter_by_frequency(reply_conv_test, min_freq=3)\n",
    "complete_filtered = post_feats_te + post_feats_tr + reply_feats_te + reply_feats_tr\n",
    "complete_filtered = helpers.flatten(complete_filtered)\n",
    "counter_filtered = Counter(complete_filtered)\n",
    "print(np.asarray([1 for k in counter_filtered]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "complete_tokens = post_conv_test + reply_conv_test + post_conv_train + reply_conv_train\n",
    "complete_tokens = helpers.flatten(complete_tokens)\n",
    "counter = Counter(complete_tokens)\n",
    "helpers.save_to_disk(counter, 'data/counter_lower.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Word types: \", np.asarray([1 for k in counter]).sum())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "vector_file = 'data/word_vectors/glove/glove.6B.50d.txt'\n",
    "word_list, vectors = dh.load_word_vectors(vector_file, 400000, 50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "word_idx = helpers.idx_lookup_from_list(word_list)\n",
    "vector_t = dh.conv_inner_to_tensor(vectors)\n",
    "vocab = nh.create_tt_vocab_obj(counter, word_idx, vector_t, max_size=None, min_freq=3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "assert len(vocab.itos) == len(vocab.vectors)\n",
    "assert len(vocab.itos) == 1 + len({w for w in counter if counter[w] >= 3})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(vocab.itos)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_ = helpers.flatten(post_conv_train + reply_conv_train)\n",
    "train_c = Counter(train_)\n",
    "train_ = set(train_)\n",
    "test_ = helpers.flatten(post_conv_test + reply_conv_test)\n",
    "test_c = Counter(test_)\n",
    "test_ = set(test_)\n",
    "len({w for w in train_c if train_c[w] >=3})\n",
    "test_.difference(train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "post_emb_train, reply_emb_train = conv_str_to_emb_idx('data/post_emb_train_lower.pkl'\n",
    "                                                      , 'data/reply_emb_train_lower.pkl'\n",
    "                                                      , post_conv_train\n",
    "                                                      , reply_conv_train, vocab.stoi\n",
    "                                                      , max_len=max_post_len)\n",
    "post_emb_test, reply_emb_test = conv_str_to_emb_idx('data/post_emb_test_lower.pkl'\n",
    "                                                    , 'data/reply_emb_test_lower.pkl'\n",
    "                                                    , post_conv_test\n",
    "                                                    , reply_conv_test, vocab.stoi\n",
    "                                                    , max_len=max_post_len)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "assert len(reply_emb_train) + len(reply_emb_test) == 218362\n",
    "assert len(post_emb_train) + len(post_emb_test) == 218362 // 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "train_labels, test_labels = get_labels('data/train_labels.pkl'\n",
    "                                       , 'data/test_labels.pkl'\n",
    "                                       , train['sarcasm'], test['sarcasm'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train_dims = get_length_tensor(reply_emb_train, post_emb_train)\n",
    "test_dims = get_length_tensor(reply_emb_test, post_emb_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "_ = plt.hist(train_dims[1], bins=100, range=[0, 35])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train_post_emb = torch.LongTensor(len(post_emb_train), max_post_len)\n",
    "train_reply_emb = torch.LongTensor(len(reply_emb_train), max_post_len)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train_post_emb = torch.LongTensor(10, max_post_len).zero_()\n",
    "for i in range(len(train_post_emb)):\n",
    "    end = len(post_emb_train[i])\n",
    "    train_post_emb[i][0:end] = post_emb_train[i]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(train_reply_emb)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv353",
   "language": "python",
   "name": "venv353"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
