{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "from scipy.stats import spearmanr, ttest_ind\n",
    "\n",
    "path = os.path.realpath(os.path.join('..', '..'))\n",
    "os.chdir(path)\n",
    "\n",
    "from src.preprocessing.datahandler import DataHandler\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.tools.config import Config\n",
    "from spacy.tokens import Token\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "data_dir = Path(Config.path.data_folder)\n",
    "sw_file_full = str(data_dir / 'stop_words_full_ultra.txt')\n",
    "sw_file_cut = str(data_dir / 'stop_words_cut_ultra.txt')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "dh = DataHandler()\n",
    "dh.load_train_test(str(data_dir))\n",
    "train = dh.get_train_df(deep_copy=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train['post'] = train['post'].map(lambda s: s.replace('&amp;', ' and '))\n",
    "train['post'] = train['post'].map(lambda s: s.replace('amp;', ' and '))\n",
    "train['reply'] = train['reply'].map(lambda s: s.replace('&amp;', ' and '))\n",
    "train['reply'] = train['reply'].map(lambda s: s.replace('amp;', ' and '))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train['post'] = train['post'].map(lambda s: s.replace('&nbsp;', ' '))\n",
    "train['post'] = train['post'].map(lambda s: s.replace('nbsp;', ' '))\n",
    "train['reply'] = train['reply'].map(lambda s: s.replace('&nbsp;', ' '))\n",
    "train['reply'] = train['reply'].map(lambda s: s.replace('nbsp;', ' '))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "stop_words_ultra = set(\n",
    "    [\"n't\", \"'s\", \"'m\", \"'re\", \"'ve\", \"'ll\", \"'d\", \"a\", \"a's\", \"able\", \"about\", \"above\", \"abroad\", \"acc\", \"acc.\",\n",
    "     \"according\"\n",
    "        , \"accordingly\", \"across\", \"actually\", \"ad.\", \"after\", \"afterwards\", \"again\"\n",
    "        , \"against\", \"ago\", \"ah\", \"aha\", \"ahead\", \"ain't\", \"all\", \"allow\", \"allows\"\n",
    "        , \"almost\", \"alone\", \"along\", \"alongside\", \"already\", \"also\", \"although\", \"always\"\n",
    "        , \"am\", \"amid\", \"amidst\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\"\n",
    "        , \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\"\n",
    "        , \"anywhere\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"are\", \"aren't\"\n",
    "        , \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"available\", \"away\"\n",
    "        , \"awfully\", \"b\", \"back\", \"backward\", \"backwards\", \"be\", \"became\", \"because\"\n",
    "        , \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"behind\"\n",
    "        , \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\"\n",
    "        , \"beyond\", \"bill\", \"both\", \"bottom\", \"brief\", \"but\", \"by\", \"c\", \"c'mon\", \"c's\"\n",
    "        , \"call\", \"came\", \"can\", \"can't\", \"cannot\", \"cant\", \"caption\", \"cause\", \"causes\"\n",
    "        , \"certain\", \"certainly\", \"cetera\", \"changes\", \"clearly\", \"co\", \"co-\", \"co.\", \"com\"\n",
    "        , \"come\", \"comes\", \"computer\", \"con\", \"concerning\", \"consequently\", \"consider\"\n",
    "        , \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\"\n",
    "        , \"could've\", \"couldn't\", \"couldnt\", \"course\", \"cry\", \"currently\", \"d\", \"dare\"\n",
    "        , \"daren't\", \"de\", \"dear\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\"\n",
    "        , \"did\", \"didn't\", \"different\", \"directly\", \"do\", \"does\", \"doesn't\", \"doing\", \"don\"\n",
    "        , \"don't\", \"done\", \"down\", \"downwards\", \"due\", \"during\", \"e\", \"e.g.\", \"each\", \"edu\"\n",
    "        , \"eg\", \"eight\", \"eighty\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"end\"\n",
    "        , \"ending\", \"enough\", \"entirely\", \"especially\", \"est\", \"et\", \"etc\", \"etc.\", \"etcetera\"\n",
    "        , \"even\", \"ever\", \"evermore\", \"every\", \"everybody\", \"everyone\", \"everything\"\n",
    "        , \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"f\", \"fairly\", \"far\", \"farther\"\n",
    "        , \"few\", \"fewer\", \"fifteen\", \"fifth\", \"fifty\", \"fify\", \"fill\", \"find\", \"fire\", \"first\"\n",
    "        , \"five\", \"followed\", \"following\", \"follows\", \"for\", \"forever\", \"former\", \"formerly\"\n",
    "        , \"forth\", \"forty\", \"forward\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\"\n",
    "        , \"furthermore\", \"g\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"go\", \"goes\"\n",
    "        , \"going\", \"gone\", \"got\", \"gotten\", \"greetings\", \"h\", \"had\", \"hadn't\", \"half\"\n",
    "        , \"happens\", \"hardly\", \"has\", \"hasn't\", \"hasnt\", \"have\", \"haven't\", \"having\", \"he\"\n",
    "        , \"he'd\", \"he'll\", \"he's\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"here's\"\n",
    "        , \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herse\", \"herself\", \"hi\"\n",
    "        , \"him\", \"himse\", \"himself\", \"his\", \"hither\", \"hm\", \"hmm\", \"hmmm\", \"hopefully\"\n",
    "        , \"how\", \"how'd\", \"how'll\", \"how's\", \"howbeit\", \"however\", \"hundred\", \"i\", \"i.e.\"\n",
    "        , \"i'd\", \"i'll\", \"i'm\", \"i've\", \"id\", \"ie\", \"if\", \"ignored\", \"immediate\", \"in\"\n",
    "        , \"inasmuch\", \"inc\", \"inc.\", \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"inner\"\n",
    "        , \"inside\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"is\", \"isn't\", \"it\"\n",
    "        , \"it'd\", \"it'll\", \"it's\", \"its\", \"itse\", \"itself\", \"j\", \"just\", \"k\", \"keep\"\n",
    "        , \"keeps\", \"kept\", \"know\", \"known\", \"knows\", \"l\", \"last\", \"lately\", \"later\", \"latter\"\n",
    "        , \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"let's\", \"like\", \"liked\", \"likely\"\n",
    "        , \"likewise\", \"little\", \"ll\", \"look\", \"looking\", \"looks\", \"low\", \"lower\", \"ltd\"\n",
    "        , \"m\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mayn't\", \"me\"\n",
    "        , \"mean\", \"meantime\", \"meanwhile\", \"merely\", \"might\", \"might've\", \"mightn't\"\n",
    "        , \"mill\", \"mine\", \"minus\", \"miss\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\"\n",
    "        , \"mr\", \"mrs\", \"much\", \"must\", \"must've\", \"mustn't\", \"my\", \"myse\", \"myself\", \"n\"\n",
    "        , \"name\", \"namely\", \"nd\", \"near\", \"nearly\", \"necessary\", \"need\", \"needn't\", \"needs\"\n",
    "        , \"neither\", \"never\", \"neverf\", \"neverless\", \"nevertheless\", \"new\", \"next\", \"nine\"\n",
    "        , \"ninety\", \"no\", \"no-one\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\"\n",
    "        , \"normally\", \"not\", \"nothing\", \"notwithstanding\", \"novel\", \"now\", \"nowhere\", \"o\"\n",
    "        , \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\"\n",
    "        , \"one's\", \"ones\", \"only\", \"onto\", \"opposite\", \"or\", \"other\", \"others\", \"otherwise\"\n",
    "        , \"ought\", \"oughtn't\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\"\n",
    "        , \"own\", \"p\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\"\n",
    "        , \"please\", \"plus\", \"possible\", \"presumably\", \"probably\", \"provided\", \"provides\", \"put\"\n",
    "        , \"q\", \"que\", \"quite\", \"qv\", \"r\", \"rather\", \"rd\", \"re\", \"really\", \"reasonably\"\n",
    "        , \"recent\", \"recently\", \"regarding\", \"regardless\", \"regards\", \"relatively\"\n",
    "        , \"respectively\", \"right\", \"round\", \"s\", \"said\", \"same\", \"saw\", \"say\", \"saying\"\n",
    "        , \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\"\n",
    "        , \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\"\n",
    "        , \"seven\", \"several\", \"shall\", \"shan\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\"\n",
    "        , \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"show\", \"side\", \"since\", \"sincere\"\n",
    "        , \"six\", \"sixty\", \"so\", \"some\", \"somebody\", \"someday\", \"somehow\", \"someone\"\n",
    "        , \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\"\n",
    "        , \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\", \"sup\", \"sure\"\n",
    "        , \"system\", \"t\", \"t's\", \"take\", \"taken\", \"taking\", \"tell\", \"ten\", \"tends\", \"th\"\n",
    "        , \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"that's\", \"that've\"\n",
    "        , \"thats\", \"the\", \"thee\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\"\n",
    "        , \"there\", \"there'd\", \"there'll\", \"there're\", \"there's\", \"there've\", \"thereafter\"\n",
    "        , \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\"\n",
    "        , \"they'd\", \"they'll\", \"they're\", \"they've\", \"thick\", \"thin\", \"thing\", \"things\"\n",
    "        , \"think\", \"third\", \"thirty\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\"\n",
    "        , \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"thx\", \"thy\", \"till\"\n",
    "        , \"tis\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tried\", \"tries\"\n",
    "        , \"truly\", \"try\", \"trying\", \"twas\", \"twelve\", \"twenty\", \"twice\", \"two\", \"u\", \"un\"\n",
    "        , \"under\", \"underneath\", \"undoing\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\"\n",
    "        , \"until\", \"unto\", \"up\", \"upon\", \"upwards\", \"us\", \"use\", \"used\", \"useful\", \"uses\"\n",
    "        , \"using\", \"usually\", \"uucp\", \"v\", \"value\", \"various\", \"ve\", \"versus\", \"very\", \"via\"\n",
    "        , \"viz\", \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasn't\", \"way\", \"we\", \"we'd\", \"we'll\"\n",
    "        , \"we're\", \"we've\", \"welcome\", \"well\", \"went\", \"were\", \"weren't\", \"what\", \"what'd\"\n",
    "        , \"what'll\", \"what's\", \"what've\", \"whatever\", \"when\", \"when'd\", \"when'll\", \"when's\"\n",
    "        , \"whence\", \"whenever\", \"where\", \"where'd\", \"where'll\", \"where's\", \"whereafter\"\n",
    "        , \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\"\n",
    "        , \"whichever\", \"while\", \"whilst\", \"whither\", \"who\", \"who'd\", \"who'll\", \"who's\"\n",
    "        , \"whoever\", \"whole\", \"whom\", \"whomever\", \"whose\", \"why\", \"why'd\", \"why'll\", \"why's\"\n",
    "        , \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"won't\", \"wonder\", \"would\"\n",
    "        , \"would've\", \"wouldn't\", \"x\", \"y\", \"yeah\", \"yes\", \"yet\", \"you\", \"you'd\", \"you'll\"\n",
    "        , \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"z\", \"zero\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "STOP_WORDS = STOP_WORDS.union(stop_words_ultra)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(STOP_WORDS)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "stop_words_getter = lambda token: token.is_stop or token.lower_ in STOP_WORDS or token.lemma_ in STOP_WORDS\n",
    "Token.set_extension('is_stop', getter=stop_words_getter)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df = train[train['sarcasm'] == 1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "doc_list = [doc for doc in nlp.pipe(df['reply'], n_threads=-1, batch_size=500)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sarc_stop_words = {}\n",
    "for word in list(STOP_WORDS):\n",
    "    sarc_stop_words[word] = 0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sarc_words = 0\n",
    "for doc in doc_list:\n",
    "    for token in doc:\n",
    "        sarc_words += 1\n",
    "        if token._.is_stop:\n",
    "            if token.lower_ in sarc_stop_words:\n",
    "                sarc_stop_words[token.lower_] += 1\n",
    "            elif token.lemma_ in sarc_stop_words:\n",
    "                sarc_stop_words[token.lemma_] += 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df = train[train['sarcasm'] == 0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "doc_list = [doc for doc in nlp.pipe(df['reply'], n_threads=-1, batch_size=500)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "norm_stop_words = {}\n",
    "for word in list(STOP_WORDS):\n",
    "    norm_stop_words[word] = 0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "norm_words = 0\n",
    "for doc in doc_list:\n",
    "    for token in doc:\n",
    "        norm_words += 1\n",
    "        if token._.is_stop:\n",
    "            if token.lower_ in sarc_stop_words:\n",
    "                norm_stop_words[token.lower_] += 1\n",
    "            elif token.lemma_ in sarc_stop_words:\n",
    "                norm_stop_words[token.lemma_] += 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "word_count = 0\n",
    "for row in df['reply']:\n",
    "    for word in row:\n",
    "        word_count += 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "norm_words"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sarc_words"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "key_list = sarc_stop_words.keys()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sarc = np.asarray([sarc_stop_words[key] for key in key_list], dtype=np.float64)\n",
    "norm = np.asarray([norm_stop_words[key] for key in key_list], dtype=np.float64)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "norm_mean = norm.mean()\n",
    "sarc_mean = sarc.mean()\n",
    "norm_std = norm.std()\n",
    "sarc_std = sarc.std()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sarc_nld = (sarc - sarc.min()) / (sarc.max() - sarc.min())\n",
    "norm_nld = (norm - norm.min()) / (norm.max() - norm.min())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Spearman:  \", spearmanr(sarc, norm))\n",
    "print(\"t-test ind:\", ttest_ind(sarc, norm))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.hist(sarc, bins='auto')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.hist(sarc_nld, bins='auto')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.hist(norm, bins='auto')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.hist(norm_nld, bins='auto')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sn_diff = np.abs((sarc_nld - norm_nld))\n",
    "plt.hist(sn_diff, bins='auto')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sarc_nld.mean()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "norm_nld.mean()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "keep_ar = [(True if num > sarc_nld.mean() else False) for num in sn_diff]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "norm2 = np.asarray([num for i, num in enumerate(norm) if keep_ar[i]], dtype=np.float64)\n",
    "sarc2 = np.asarray([num for i, num in enumerate(sarc) if keep_ar[i]], dtype=np.float64)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.hist(norm2, bins='auto')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.hist(sarc2, bins='auto')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Spearman:  \", spearmanr(sarc2, norm2))\n",
    "print(\"t-test ind:\", ttest_ind(sarc2, norm2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sn_diff = np.abs((sarc2 - norm2))\n",
    "plt.hist(sn_diff, bins='auto')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "words_to_keep = [key for i, key in enumerate(key_list) if keep_ar[i]]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "words_to_keep"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "np.savetxt(sw_file_full, np.asarray(list(STOP_WORDS)), fmt='%s')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(STOP_WORDS)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "for word in words_to_keep:\n",
    "    try:\n",
    "        STOP_WORDS.remove(word)\n",
    "    except:\n",
    "        pass"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(STOP_WORDS)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "np.savetxt(sw_file_cut, np.asarray(list(STOP_WORDS)), fmt='%s')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "norm_stop_words"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
