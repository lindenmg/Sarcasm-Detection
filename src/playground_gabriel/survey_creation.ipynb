{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey Creation\n",
    "This Notebook creates a survey where the replies have a similar distribution of word length then in the whole test set. The data for the survey is taken from the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.path.realpath(os.path.join('..', '..'))\n",
    "os.chdir(path)\n",
    "\n",
    "from src.preprocessing.preprocessing import Preprocessing\n",
    "from src.preprocessing.datahandler import DataHandler\n",
    "from pathlib import Path\n",
    "\n",
    "import src.tools.helpers as helpers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "random_seed = 1337659\n",
    "survey_file = \"data/survey.csv\"\n",
    "survey_solution = \"data/survey_solution.csv\"\n",
    "sample_number = 40  # The number of post-reply pairs for the survey\n",
    "np.random.seed(random_seed)\n",
    "sw_cut_file = 'data/stop_words_cut_ultra.txt'\n",
    "sw_full_file = 'data/stop_words_full_ultra.txt'\n",
    "filter_stop_words = False\n",
    "filter_punctuation = False\n",
    "dh = DataHandler()\n",
    "dh.load_train_test('data/')\n",
    "test = dh.get_test_df(deep_copy=False)\n",
    "\n",
    "## We don't want to have a post twice in the survey data\n",
    "test = dh.shuffle_post_pairs(test)\n",
    "test = test[0::2]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "pp = Preprocessing(model_type='en')\n",
    "nlp = pp.get_nlp()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_spacy_pipeline(post_path, reply_path, df):\n",
    "    post_dump = Path(post_path)\n",
    "    reply_dump = Path(reply_path)\n",
    "    if not post_dump.is_file():\n",
    "        posts = pp.run_spacy_pipeline(df['post'][0::2])\n",
    "        helpers.save_to_disk(posts, post_path)\n",
    "    else:\n",
    "        posts = helpers.load_from_disk(post_path)\n",
    "    if not reply_dump.is_file():\n",
    "        replies = pp.run_spacy_pipeline(df['reply'])\n",
    "        helpers.save_to_disk(replies, reply_path)\n",
    "    else:\n",
    "        replies = helpers.load_from_disk(reply_path)\n",
    "    return posts, replies\n",
    "\n",
    "\n",
    "def apply_token_to_x(post_path, reply_path, posts, replies, type_):\n",
    "    post_dump = Path(post_path)\n",
    "    reply_dump = Path(reply_path)\n",
    "    if not post_dump.is_file():\n",
    "        nlp.add_stop_word_def(sw_full_file)\n",
    "        post_docs = pp.filter_spacy_tokens(posts, no_stop_words=False, no_punctuation=False)\n",
    "        post_pcd = pp.convert_token_docs_text(post_docs, token_kind=type_, transform_specials=True)\n",
    "        helpers.save_to_disk(post_pcd, post_path)\n",
    "    else:\n",
    "        post_pcd = helpers.load_from_disk(post_path)\n",
    "    if not reply_dump.is_file():\n",
    "        nlp.add_stop_word_def(sw_cut_file)\n",
    "        reply_docs = pp.filter_spacy_tokens(replies, no_stop_words=False, no_punctuation=False)\n",
    "        reply_pcd = pp.convert_token_docs_text(reply_docs, token_kind=type_, transform_specials=True)\n",
    "        helpers.save_to_disk(reply_pcd, reply_path)\n",
    "    else:\n",
    "        reply_pcd = helpers.load_from_disk(reply_path)\n",
    "    return post_pcd, reply_pcd\n",
    "\n",
    "\n",
    "def create_length_probability_series(plot):\n",
    "    plot_length = len(plot[0])\n",
    "    index = np.zeros(plot_length)\n",
    "    data = np.zeros_like(index)\n",
    "    sum_ = plot[0].sum()\n",
    "    for i, amount in enumerate(plot[0]):\n",
    "        index[i] = i + 1\n",
    "        data[i] = amount / sum_\n",
    "    return pd.Series(data=data, index=index)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "posts_test, reply_test = apply_spacy_pipeline('data/posts_test_survey.pkl', 'data/replies_test_survey.pkl', test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "post_lower, reply_lower = apply_token_to_x('data/post_lower_test_survey.pkl'\n",
    "                                           , 'data/reply_lower_test_survey.pkl'\n",
    "                                           , posts_test, reply_test, 'lower_')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "post_lengths = [len(doc) for doc in post_lower]\n",
    "reply_lengths = [len(doc) for doc in reply_lower]\n",
    "reply_lower[9]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Reply sentence lengths distribution\")\n",
    "plot = plt.hist(reply_lengths, bins=30, range=[1, 30])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "post_lengths = np.asarray(post_lengths)\n",
    "reply_lengths = np.asarray(reply_lengths)\n",
    "print(\"Standard deviation of reply sentence lengths: {:.1f}\".format(reply_lengths.std()))\n",
    "print(\"Mean of reply sentence lengths:               {:.1f}\".format(reply_lengths.mean()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "base_data = test\n",
    "base_data.index = reply_lengths\n",
    "sample_weights = create_length_probability_series(plot)\n",
    "sample_weights.values.sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sample = base_data.sample(sample_number, random_state=random_seed, weights=sample_weights)\n",
    "_ = plt.hist(sample.index, bins=40, range=[1, 30])\n",
    "sample[\"sarcasm\"].sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "survey_data = sample[[\"post\", \"reply\"]]\n",
    "survey_data.to_csv(survey_file, index=False)\n",
    "sample.to_csv(survey_solution)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
