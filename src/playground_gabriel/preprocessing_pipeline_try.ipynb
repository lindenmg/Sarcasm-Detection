{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "path = os.path.realpath(os.path.join('..', '..'))\n",
    "os.chdir(path)\n",
    "\n",
    "from src.preprocessing.preprocessing import Preprocessing\n",
    "from src.data_science.networkhelper import NetworkHelper\n",
    "from src.preprocessing.datahandler import DataHandler\n",
    "\n",
    "from pathlib import Path\n",
    "import src.tools.helpers as helpers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have to delete every file with _emb_ and counter in it after test_file name change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "test_file = \"survey_test\"  # Write NO file ending !!!\n",
    "test_file_data_num = 40"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_train_test(data_dir, dh, file_to_test):\n",
    "    train_file = data_dir / \"train.csv\"\n",
    "    test_file = data_dir / (file_to_test + \".csv\")\n",
    "    if not (train_file.is_file()):\n",
    "        comments = str(data_dir / \"comments_cleaned.txt\")\n",
    "        annotation = str(data_dir / \"annotation.txt\")\n",
    "        dh.load_data(comments, annotation)\n",
    "        dh.split_in_train_test()\n",
    "        dh.save_train_test_to_csv(str(data_dir))\n",
    "    else:\n",
    "        dh.load_train_test(str(data_dir))\n",
    "    train = dh.get_train_df(deep_copy=False)\n",
    "    if file_to_test == \"test\":\n",
    "        test = dh.get_test_df(deep_copy=False)\n",
    "    else:\n",
    "        dtype = {'post_id': np.str, 'post': np.str\n",
    "            , 'reply': np.str, 'sarcasm': np.int8}\n",
    "        test = pd.read_csv(test_file, sep='\\t', keep_default_na=False\n",
    "                           , na_values=\"\", dtype=dtype)\n",
    "    return test, train"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "data_dir = Path('data')\n",
    "sw_cut_file = str(data_dir / 'stop_words_cut_ultra.txt')\n",
    "sw_full_file = str(data_dir / 'stop_words_full_ultra.txt')\n",
    "nh = NetworkHelper()\n",
    "dh = DataHandler()\n",
    "test, train = load_train_test(data_dir, dh, test_file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "max_post_len = 50"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "pp = Preprocessing()\n",
    "nlp = pp.get_nlp()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_spacy_pipeline(post_path, reply_path, df):\n",
    "    post_dump = Path(post_path)\n",
    "    reply_dump = Path(reply_path)\n",
    "    if not post_dump.is_file():\n",
    "        posts = pp.run_spacy_pipeline(df['post'][0::2])\n",
    "        helpers.save_to_disk(posts, post_path)\n",
    "    else:\n",
    "        posts = helpers.load_from_disk(post_path)\n",
    "    if not reply_dump.is_file():\n",
    "        replies = pp.run_spacy_pipeline(df['reply'])\n",
    "        helpers.save_to_disk(replies, reply_path)\n",
    "    else:\n",
    "        replies = helpers.load_from_disk(reply_path)\n",
    "    return posts, replies\n",
    "\n",
    "\n",
    "def apply_token_to_x(post_path, reply_path, posts, replies, type_):\n",
    "    post_dump = Path(post_path)\n",
    "    reply_dump = Path(reply_path)\n",
    "    if not post_dump.is_file():\n",
    "        nlp.add_stop_word_def(sw_full_file)\n",
    "        post_docs = pp.filter_spacy_tokens(posts, no_stop_words=False, no_punctuation=False)\n",
    "        post_pcd = pp.convert_token_docs_text(post_docs, token_kind=type_, transform_specials=True)\n",
    "        helpers.save_to_disk(post_pcd, post_path)\n",
    "    else:\n",
    "        post_pcd = helpers.load_from_disk(post_path)\n",
    "    if not reply_dump.is_file():\n",
    "        nlp.add_stop_word_def(sw_cut_file)\n",
    "        reply_docs = pp.filter_spacy_tokens(replies, no_stop_words=False, no_punctuation=False)\n",
    "        reply_pcd = pp.convert_token_docs_text(reply_docs, token_kind=type_, transform_specials=True)\n",
    "        helpers.save_to_disk(reply_pcd, reply_path)\n",
    "    else:\n",
    "        reply_pcd = helpers.load_from_disk(reply_path)\n",
    "    return post_pcd, reply_pcd\n",
    "\n",
    "\n",
    "def conv_str_to_emb_idx(post_path, reply_path, posts, replies, word_idx, max_len=1000):\n",
    "    post_dump = Path(post_path)\n",
    "    reply_dump = Path(reply_path)\n",
    "    if not (post_dump.is_file() and reply_dump.is_file()):\n",
    "        post_emb = nh.convert_str_to_emb_idx(posts, word_idx, max_len)\n",
    "        reply_emb = nh.convert_str_to_emb_idx(replies, word_idx, max_len)\n",
    "        helpers.save_to_disk(post_emb, post_path)\n",
    "        helpers.save_to_disk(reply_emb, reply_path)\n",
    "    else:\n",
    "        post_emb = helpers.load_from_disk(post_path)\n",
    "        reply_emb = helpers.load_from_disk(reply_path)\n",
    "    return post_emb, reply_emb\n",
    "\n",
    "\n",
    "def get_labels(train_path, test_path, train, test):\n",
    "    train_dump = Path(train_path)\n",
    "    test_dump = Path(test_path)\n",
    "    if not (train_dump.is_file() and test_dump.is_file()):\n",
    "        train = train.values.astype(dtype=np.long, copy=False)\n",
    "        test = test.values.astype(dtype=np.long, copy=False)\n",
    "        train = torch.from_numpy(train)\n",
    "        test = torch.from_numpy(test)\n",
    "        helpers.save_to_disk(train, train_path)\n",
    "        helpers.save_to_disk(test, test_path)\n",
    "    else:\n",
    "        train = helpers.load_from_disk(train_path)\n",
    "        test = helpers.load_from_disk(test_path)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def get_length_tensor(replies, posts):\n",
    "    reply_length = helpers.create_length_tensor(replies)\n",
    "    post_length = helpers.create_length_tensor(posts)\n",
    "    return post_length, reply_length"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "posts_train, reply_train = apply_spacy_pipeline('data/posts.pkl', 'data/replies.pkl', train)\n",
    "# posts_train = None\n",
    "# reply_train = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "posts_test, reply_test = apply_spacy_pipeline('data/posts_' + test_file + '.pkl'\n",
    "                                              , 'data/replies_' + test_file + '.pkl', test)\n",
    "# reply_test = None\n",
    "# posts_test = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "post_conv_train, reply_conv_train = apply_token_to_x('data/post_lower.pkl', 'data/reply_lower.pkl'\n",
    "                                                     , posts_train, reply_train, 'lower_')\n",
    "post_conv_test, reply_conv_test = apply_token_to_x('data/post_lower_' + test_file + '.pkl'\n",
    "                                                   , 'data/reply_lower_' + test_file + '.pkl'\n",
    "                                                   , posts_test, reply_test, 'lower_')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "post_conv_train, reply_conv_train = apply_token_to_x('data/post_text_train.pkl', 'data/reply_text_train.pkl'\n",
    "                                                           , posts_train, reply_train, 'text')\n",
    "post_conv_test, reply_conv_test = apply_token_to_x('data/post_text_test.pkl'\n",
    "                                                         , 'data/reply_text_test.pkl'\n",
    "                                                         , posts_test, reply_test, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "post_feats_tr, _ = pp.filter_by_frequency(post_conv_train, min_freq=3)\n",
    "reply_feats_tr, _ = pp.filter_by_frequency(reply_conv_train, min_freq=3)\n",
    "post_feats_te, _ = pp.filter_by_frequency(post_conv_test, min_freq=3)\n",
    "reply_feats_te, _ = pp.filter_by_frequency(reply_conv_test, min_freq=3)\n",
    "complete_filtered = post_feats_te + post_feats_tr + reply_feats_te + reply_feats_tr\n",
    "complete_filtered = helpers.flatten(complete_filtered)\n",
    "counter_filtered = Counter(complete_filtered)\n",
    "print(np.asarray([1 for k in counter_filtered]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "complete_tokens = post_conv_test + reply_conv_test + post_conv_train + reply_conv_train\n",
    "complete_tokens = helpers.flatten(complete_tokens)\n",
    "counter = Counter(complete_tokens)\n",
    "helpers.save_to_disk(counter, 'data/counter_lower.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "post_conv_train"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Word types: \", len(counter))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "vector_file = 'data/word_vectors/fastText/ft_2M_300.csv'\n",
    "word_list, vectors = dh.load_word_vectors(vector_file, int(2e6 - 1), 300)\n",
    "word_idx = helpers.idx_lookup_from_list(word_list)\n",
    "vector_t = dh.conv_inner_to_tensor(vectors)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "vector_file = 'data/word_vectors/word2vec/GoogleNews-vectors-negative300.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(vector_file, binary=True)\n",
    "word_idx = helpers.idx_lookup_from_list(model.index2word)\n",
    "vector_t = dh.conv_inner_to_tensor(model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "vocab = nh.create_tt_vocab_obj(counter, word_idx, vector_t, max_size=None, min_freq=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "assert len(vocab.itos) == len(vocab.vectors)\n",
    "assert len(vocab.itos) <= 1 + len({w for w in counter if counter[w] >= 1})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "length = 0\n",
    "dict_ = vocab.stoi\n",
    "for k in dict_:\n",
    "    if dict_[k] != -1:\n",
    "        length += 1\n",
    "length"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_ = helpers.flatten(post_conv_train + reply_conv_train)\n",
    "train_c = Counter(train_)\n",
    "train_ = set(train_)\n",
    "test_ = helpers.flatten(post_conv_test + reply_conv_test)\n",
    "test_c = Counter(test_)\n",
    "test_ = set(test_)\n",
    "len({w for w in train_c if train_c[w] >=3})\n",
    "test_.difference(train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "post_emb_train, reply_emb_train = conv_str_to_emb_idx('data/post_emb_train_lower.pkl'\n",
    "                                                      , 'data/reply_emb_train_lower.pkl'\n",
    "                                                      , post_conv_train\n",
    "                                                      , reply_conv_train, vocab.stoi\n",
    "                                                      , max_len=max_post_len)\n",
    "post_emb_test, reply_emb_test = conv_str_to_emb_idx('data/post_emb_' + test_file + '_lower.pkl'\n",
    "                                                    , 'data/reply_emb_' + test_file + '_lower.pkl'\n",
    "                                                    , post_conv_test\n",
    "                                                    , reply_conv_test, vocab.stoi\n",
    "                                                    , max_len=max_post_len)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if test_file == \"test\":\n",
    "    data_count = 196526 + test_file_data_num\n",
    "    assert len(reply_emb_train) + len(reply_emb_test) == data_count\n",
    "    assert len(post_emb_train) + len(post_emb_test) == data_count // 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "train_labels, test_labels = get_labels('data/train_labels.pkl'\n",
    "                                       , 'data/' + test_file + '_labels.pkl'\n",
    "                                       , train['sarcasm'], test['sarcasm'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train_dims = get_length_tensor(reply_emb_train, post_emb_train)\n",
    "test_dims = get_length_tensor(reply_emb_test, post_emb_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "_ = plt.hist(train_dims[1], bins=100, range=[0, 50])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train_post_emb = torch.LongTensor(len(post_emb_train), max_post_len)\n",
    "train_reply_emb = torch.LongTensor(len(reply_emb_train), max_post_len)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train_post_emb = torch.LongTensor(10, max_post_len).zero_()\n",
    "for i in range(len(train_post_emb)):\n",
    "    end = len(post_emb_train[i])\n",
    "    train_post_emb[i][0:end] = post_emb_train[i]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(train_reply_emb)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "len(reply_emb_test)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
