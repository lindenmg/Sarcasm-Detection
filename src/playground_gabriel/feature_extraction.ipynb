{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-Extraktion from CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we extract the values before they normally would be forwarded to the final fully connected layer. We use the values at this semi-final stage as features for the Machine Learning classifiers which are no Neural Networks. The values will be calculated for the training- and test-set with a fully trained CNN with attention. The (dis-)advatange of the CNN with attention ist, that it hast half the feature count then the normal CNN. The CNN has been trained beforehand on the training-set. To generate the features it basically runs in test-mode. Which means model.eval() and volatile=true for the input autograd.Variables in case of PyTorch which is used here. The generated values are saved as a CSV file. Each column in the CSV file stands for a result of the 1-max-pooling which is currently used. The actual training of the ML classfiers will happen in a different notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "path = os.path.realpath(os.path.join('..', '..'))\n",
    "os.chdir(path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm  # _notebook as tqdm\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "from src.training.learning_session import LearningSession\n",
    "from src.tools.config import Config"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we would have to load the JSON config-file of the CNN anyway because of the name of the saved network-files, we will just use the LearningSession class in an irregular way. That doesn't lead to full function code clones and saves some work. This goes also for the preprocessed data for the CNN input. You have to be aware of the fact, that the config file should contain the actual params of the the training session you want to load your model from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "train_pipe_config_path = Path(Config.path.project_root_folder) / 'src' / 'strategies'\n",
    "train_pipe_config_path = train_pipe_config_path / 'cnn' / 'attentive_cnn_config.json'\n",
    "data_folder = Path(Config.path.data_folder)\n",
    "train_file = data_folder / \"features_train_i.csv\"\n",
    "test_file = data_folder / \"features_test_i.csv\"\n",
    "\n",
    "with open(str(train_pipe_config_path)) as file:\n",
    "    params = json.load(file)\n",
    "\n",
    "logger_args = {\n",
    "    'tensorboard_log_dir': Config.path.log_folder,\n",
    "    'mongo_host': 'localhost',\n",
    "    'mongo_port': Config.logging.port\n",
    "}\n",
    "params['logger']['args'].update(logger_args)\n",
    "params['learning_session'].update({'cache_folder': Config.path.cache_folder})\n",
    "ls = LearningSession(params)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "result_dict = ls.data_factory.get_data()\n",
    "train_datadict = result_dict['train_data']\n",
    "test_datadict = result_dict['test_data']\n",
    "reply_lengths = result_dict.get('reply_lengths', None)\n",
    "ls.word_vectors = result_dict.get('word_vectors', None)\n",
    "model = ls._load_saved_model(fold=1, mode=\"tr_full\")\n",
    "\n",
    "if model is None:\n",
    "    raise Warning(\"Loading of model has failed, there seems to be no model with given parameters\")\n",
    "\n",
    "# We MUST change the flag for the feature extraction after loading\n",
    "if not hasattr(model, 'log_features'):\n",
    "    raise AttributeError(\"Model does not have log_features attribute, wrong model type!\")\n",
    "model.log_features = True"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "key = 'cv_iterator_factory'\n",
    "args = {'reply_lengths': reply_lengths}\n",
    "ls.args[key] = ls._update_args(key, args)\n",
    "ls._load_cv_iterator_factory()\n",
    "train_data_loader = ls._create_dataloader(data_dict=train_datadict, reply_lengths=None)\n",
    "test_data_loader = ls._create_dataloader(data_dict=test_datadict, reply_lengths=None)\n",
    "train_data_loader.shuffle = False\n",
    "test_data_loader.shuffle = False"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Numpy array get feat_num + 1, because of the label column\n",
    "feat_num = params[\"model\"][\"args\"][\"hl1_kernel_num\"]\n",
    "features_train = np.zeros((196526, feat_num + 1), dtype='float32')\n",
    "features_test = np.zeros((21836, feat_num + 1), dtype='float32')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "has_cuda = torch.cuda.is_available()\n",
    "column_names = [str(i) for i in range(feat_num)]\n",
    "column_names.append(\"label\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if has_cuda:\n",
    "    model.cuda()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The functions we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_features(dataloader, model, feat_num, feature_table):\n",
    "    def _step(variable_dict, labels):\n",
    "        features = model(**variable_dict)\n",
    "        batch_size = len(variable_dict[\"replies\"])\n",
    "        feature_table[i:i + batch_size, :feat_num] = features.squeeze().data.cpu().numpy()\n",
    "        feature_table[i:i + batch_size, feat_num] = labels.cpu().numpy()\n",
    "\n",
    "    # For the increase of the index i. Only the last batch will probabaly be different, which is no problem\n",
    "    batch_size = dataloader.batch_sampler.batch_size\n",
    "    model.eval()\n",
    "    i = 0\n",
    "\n",
    "    if has_cuda:\n",
    "        for data, labels in tqdm(dataloader):\n",
    "            variable_dict = {k: Variable(v, volatile=True).cuda() for k, v in data.items()}\n",
    "            _step(variable_dict, labels)\n",
    "            i += batch_size\n",
    "    else:\n",
    "        for data, labels in tqdm(dataloader):\n",
    "            variable_dict = {k: Variable(v, volatile=True) for k, v in data.items()}\n",
    "            _step(variable_dict, labels)\n",
    "            i += batch_size\n",
    "    return feature_table"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This till take some seconds:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "features_train = extract_features(train_data_loader, model, feat_num, features_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "features_test = extract_features(test_data_loader, model, feat_num, features_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "train_df = pd.DataFrame(data=features_train, dtype='float32', columns=column_names)\n",
    "test_df = pd.DataFrame(data=features_test, dtype='float32', columns=column_names)\n",
    "train_df.to_csv(str(train_file), index=False)\n",
    "test_df.to_csv(str(test_file), index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_Jupyter",
   "language": "python",
   "name": "python3_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
