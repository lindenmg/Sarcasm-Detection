{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of text lengths\n",
    "We want to take a look at the word lengths of the posts and replies and their distribution. With filtering of unfrequent words and without that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.path.realpath(os.path.join('..', '..'))\n",
    "os.chdir(path)\n",
    "\n",
    "from src.tools.helpers import load_from_disk, save_to_disk\n",
    "from src.preprocessing.preprocessing import Preprocessing\n",
    "from src.preprocessing.datahandler import DataHandler\n",
    "from pathlib import Path\n",
    "\n",
    "import src.tools.helpers as helpers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sw_cut_file = 'data/stop_words_cut_ultra.txt'\n",
    "sw_full_file = 'data/stop_words_full_ultra.txt'\n",
    "filter_stop_words = False\n",
    "filter_punctuation = False\n",
    "dh = DataHandler()\n",
    "dh.load_train_test('data/')\n",
    "df = dh.get_train_df(deep_copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pp = Preprocessing(model_type='en')\n",
    "nlp = pp.get_nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "post_dump = Path('data/post_lower.pkl')\n",
    "reply_dump = Path('data/reply_lower.pkl')\n",
    "if not (post_dump.is_file() and reply_dump.is_file()):\n",
    "    post_dump = Path('data/posts.pkl')\n",
    "    reply_dump = Path('data/replies.pkl')\n",
    "    if not post_dump.is_file():\n",
    "        posts = pp.run_spacy_pipeline(df['post'][0::2])\n",
    "        save_to_disk(posts, 'data/posts.pkl')\n",
    "    else:\n",
    "        posts = load_from_disk('data/posts.pkl')\n",
    "    if not reply_dump.is_file():\n",
    "        replies = pp.run_spacy_pipeline(df['reply'])\n",
    "        save_to_disk(replies, 'data/replies.pkl')\n",
    "    else:\n",
    "        replies = load_from_disk('data/replies.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "post_dump = Path('data/post_lower.pkl')\n",
    "reply_dump = Path('data/reply_lower.pkl')\n",
    "if not post_dump.is_file():\n",
    "    nlp.add_stop_word_def(sw_full_file)\n",
    "    post_docs = pp.filter_spacy_tokens(posts, no_stop_words=filter_stop_words\n",
    "                                       , no_punctuation=filter_punctuation)\n",
    "    post_lower = pp.convert_token_docs_text(post_docs, transform_specials=True)\n",
    "    save_to_disk(post_lower, 'data/post_lower.pkl')\n",
    "else:\n",
    "    post_lower = load_from_disk('data/post_lower.pkl')\n",
    "if not reply_dump.is_file():\n",
    "    nlp.add_stop_word_def(sw_cut_file)\n",
    "    reply_docs = pp.filter_spacy_tokens(replies, no_stop_words=filter_stop_words\n",
    "                                        , no_punctuation=filter_punctuation)\n",
    "    reply_lower = pp.convert_token_docs_text(reply_docs, transform_specials=True)\n",
    "    save_to_disk(reply_lower, 'data/reply_lower.pkl')\n",
    "else:\n",
    "    reply_lower = load_from_disk('data/reply_lower.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is without word filtering based on frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lengths = [len(doc) for doc in post_lower]\n",
    "reply_lengths = [len(doc) for doc in reply_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_tokens = [word for doc in reply_lower for word in doc]\n",
    "post_tokens = [word for doc in post_lower for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(post_lengths, bins=100, range=[0, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lengths_ar = np.asarray(post_lengths)\n",
    "word_types = len(set(post_tokens))\n",
    "print(\"Posts:\")\n",
    "print(\"word tokens: {:,.0f}\".format(post_lengths_ar.sum()))\n",
    "print(\"word types:  {:,.0f}\".format(word_types))\n",
    "print(\"std:         {:.2f}\".format(post_lengths_ar.std()))\n",
    "print(\"mean:        {:.2f}\".format(post_lengths_ar.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(reply_lengths, bins=100, range=[0, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_lengths_ar = np.asarray(reply_lengths)\n",
    "word_types = len(set(reply_tokens))\n",
    "print(\"Replies:\")\n",
    "print(\"word tokens: {:,.0f}\".format(reply_lengths_ar.sum()))\n",
    "print(\"word types:  {:,.0f}\".format(word_types))\n",
    "print(\"std:         {:.2f}\".format(reply_lengths_ar.std()))\n",
    "print(\"mean:        {:.2f}\".format(reply_lengths_ar.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = reply_lower + post_lower\n",
    "word_types = len(set(helpers.flatten(comments)))\n",
    "print(\"word types combined: {:,.0f}\".format(word_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_cut = [num for num in post_lengths if num <= 20]\n",
    "reply_cut = [num for num in reply_lengths if num <= 20]\n",
    "print(\"Percentage of posts for length <=20:\", (len(post_cut) / len(post_lengths)))\n",
    "print(\"Percentage of replies for length <=20:\", (len(reply_cut) / len(reply_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is with word filtering based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "post_feats, _ = pp.filter_by_frequency(post_lower, min_freq=3)\n",
    "reply_feats, _ = pp.filter_by_frequency(reply_lower, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_tokens = [word for doc in reply_feats for word in doc]\n",
    "post_tokens = [word for doc in post_feats for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lengths = [len(doc) for doc in post_feats]\n",
    "reply_lengths = [len(doc) for doc in reply_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(post_lengths, bins=100, range=[0, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lengths_ar = np.asarray(post_lengths)\n",
    "word_types = len(set(post_tokens))\n",
    "print(\"Posts:\")\n",
    "print(\"word tokens:     {:,.0f}\".format(post_lengths_ar.sum()))\n",
    "print(\"word types: {:,.0f}\".format(word_types))\n",
    "print(\"std:            {:.2f}\".format(post_lengths_ar.std()))\n",
    "print(\"mean:           {:.2f}\".format(post_lengths_ar.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(reply_lengths, bins=100, range=[0, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_lengths_ar = np.asarray(reply_lengths)\n",
    "word_types = len(set(reply_tokens))\n",
    "print(\"Replies:\")\n",
    "print(\"word tokens: {:,.0f}\".format(len(reply_tokens)))\n",
    "print(\"word types:  {:,.0f}\".format(word_types))\n",
    "print(\"std:         {:.2f}\".format(reply_lengths_ar.std()))\n",
    "print(\"mean:        {:.2f}\".format(reply_lengths_ar.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_comments, _ = pp.filter_by_frequency(reply_lower + post_lower, min_freq=3)\n",
    "word_types = len(set(helpers.flatten(filtered_comments)))\n",
    "print(\"word types combined: {:,.0f}\".format(word_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_cut = [num for num in post_lengths if num <= 30]\n",
    "reply_cut = [num for num in reply_lengths if num <= 30]\n",
    "print(\"Percentage of posts for length <=20:\", (len(post_cut) / len(post_lengths)))\n",
    "print(\"Percentage of replies for length <=20:\", (len(reply_cut) / len(reply_lengths)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
